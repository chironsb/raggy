# ğŸ¤– Raggy

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18.0.0-brightgreen)](https://nodejs.org/)

Local RAG (Retrieval-Augmented Generation) system with OpenCode integration.

## âœ¨ What it does

- ğŸ“„ Upload PDFs and text files
- ğŸ” Search through documents using natural language
- ğŸ’¬ Get answers with sources and context
- ğŸ  Everything runs locally on your machine

## ğŸš€ Quick start

```bash
git clone https://github.com/chironsb/raggy.git
cd raggy
npm install
cp .env.example .env
npm run dev
```

ğŸŒ Server starts on `http://localhost:3001`

## ğŸ“– Usage

### ğŸ¤– Via OpenCode agent

The easiest way to use Raggy is through the OpenCode RAG agent:

```bash
raggy status                             # ğŸ“Š Check server status
raggy upload /path/to/file.pdf [collection]  # â¬†ï¸ Upload a document
raggy query "your question" [collection]     # â“ Ask questions
raggy list                               # ğŸ“‹ List all collections
raggy stop                               # ğŸ›‘ Stop server
```

### ğŸŒ Via REST API

```bash
# â¬†ï¸ Upload document
curl -X POST http://localhost:3001/api/upload \
  -H "Content-Type: application/json" \
  -d '{"filePath": "/path/to/file.pdf", "collection": "docs"}'

# â“ Query documents
curl -X POST http://localhost:3001/api/query \
  -H "Content-Type: application/json" \
  -d '{"question": "What is this about?", "collection": "docs"}'
```

## ğŸ¯ Features

- ğŸ“„ **PDF & TXT processing** - Extract text and metadata
- âœ‚ï¸ **Smart chunking** - Split documents into meaningful pieces
- ğŸ§  **Local embeddings** - Uses Xenova/transformers (no API calls)
- ğŸ” **Vector search** - Fast similarity search with cosine similarity
- ğŸ’¾ **Persistent storage** - JSON-based vector database
- ğŸ“ **Collection management** - Organize documents into collections
- ğŸ”— **OpenCode integration** - Full agent support with custom tools

## âš™ï¸ Configuration

Edit `.env` to customize:

```bash
PORT=3001
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200
RAG_SIMILARITY_THRESHOLD=0.3
EMBEDDING_MODEL=Xenova/paraphrase-multilingual-MiniLM-L12-v2
```

Lower `RAG_SIMILARITY_THRESHOLD` (0.2-0.4) for more results, higher (0.6-0.8) for stricter matching.

## ğŸ“‹ Requirements

- ğŸŸ¢ Node.js 18+
- ğŸ§  2GB+ RAM (4GB recommended for large documents)

## ğŸ—ï¸ Project structure

```
src/
â”œâ”€â”€ core/          # ğŸ§  RAG logic (embeddings, chunking, vector DB)
â”œâ”€â”€ server.ts      # ğŸŒ Express API server
â”œâ”€â”€ tools/         # ğŸ”§ OpenCode integration
â””â”€â”€ index.ts       # ğŸšª Entry point

data/              # ğŸ“ Auto-created on first run
â”œâ”€â”€ vectors/       # ğŸ“Š Vector database (JSON files)
â”œâ”€â”€ documents/     # ğŸ“„ Uploaded PDFs/TXT files
â””â”€â”€ cache/         # âš¡ Embedding cache
```

## ğŸ”„ How it works

1. **â¬†ï¸ Upload** - PDF/TXT files are processed and text is extracted
2. **âœ‚ï¸ Chunk** - Text is split into overlapping chunks (default 1000 chars)
3. **ğŸ§  Embed** - Each chunk gets a vector embedding (384 dimensions)
4. **ğŸ’¾ Store** - Vectors are saved to JSON files in `data/vectors/`
5. **â“ Query** - Your question is embedded and compared to all chunks
6. **ğŸ“¤ Return** - Most similar chunks are returned with their sources

## âš¡ Performance

The system uses parallel batch processing (50 chunks at a time) for fast uploads:
- ğŸš€ ~600 chunks in 1-2 seconds
- ğŸ’» Normal CPU usage during processing
- âš¡ Results in milliseconds once indexed

## ğŸ”® Future Plans

- **GPU Acceleration** - Implement embedding generation on GPU for faster processing
- **More file formats** - Add support for DOCX, HTML, Markdown
- **Advanced chunking** - Semantic chunking based on document structure
- **Web UI** - Simple web interface for document management

## ğŸ“„ License

MIT
